---
title: "Full Code"
output: pdf_document
date: "2024-05-05"
---
```{r setup, include=FALSE}
# # # # # #
# SET UP ##
# # # # # #

knitr::opts_chunk$set(echo = TRUE)

# # # # # # # # #
## Load Libraries

library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)
library(tidyverse)

library(ggdendro)
library(factoextra)
library(gridExtra)
library(Metrics)
library(cluster)

options(repos = "https://cloud.r-project.org")
library(xgboost)
```

# Data Cleaning

## Data Combination

```{r data combination}

# # # # # # # # # 
# DATA CLEANING #
# # # # # # # # #

# # # # # # # # # # # #
## Combine 12 Datasets

"C:/Users/paige/OneDrive/Documents/STAT 472/Team-Koopa/not combined csv files"

#getwd()

setwd("C:/Users/paige/OneDrive/Documents/STAT 472/Team-Koopa/not combined csv files")

data1 <- read.csv("Criminal_Offenses_On_campus.csv") |> 
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x,"_all_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_all_campus, unique_id = unique_id_all_campus)

data2 <- read.csv("Criminal_Offenses_On_campus_Student_Housing_Facilities.csv") |> 
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |> 
  rename_with(~ paste0(.x,"_student_housing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_student_housing, unique_id = unique_id_student_housing)

data3 <- read.csv("Criminal_Offenses_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_crim_offense_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_crim_offense_noncampus, unique_id = unique_id_crim_offense_noncampus)

data4 <- read.csv("Criminal_Offenses_Public_property.csv") |>
   mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_crim_offense_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_crim_offense_public, unique_id = unique_id_crim_offense_public)
  
data5 <- read.csv("Arrests_On_campus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_campus, unique_id = unique_id_arrests_campus)

data6 <- read.csv("Arrests_On_campus_Student_Housing_Facilities.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_stuhousing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_stuhousing, unique_id = unique_id_arrests_stuhousing)
  
data7 <- read.csv("Arrests_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_noncampus, unique_id = unique_id_arrests_noncampus)
  
data8 <- read.csv("Arrests_Public_Property.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_public, unique_id = unique_id_arrests_public)
  
data9 <- read.csv("Disciplinary_Actions_On_campus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_campus, unique_id = unique_id_disciplinary_campus)

setwd("C:/Users/paige/OneDrive/Documents/STAT 472/Team-Koopa")

data10 <- read.csv("Disciplinary_Actions_Student_Housing_Facilities.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_housing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_housing, unique_id = unique_id_disciplinary_housing)

setwd("C:/Users/paige/OneDrive/Documents/STAT 472/Team-Koopa/not combined csv files")

data11 <- read.csv("Disciplinary_Actions_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_noncampus, unique_id = unique_id_disciplinary_noncampus)
  
data12 <- read.csv("Disciplinary_Actions_Public_Property.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_public, unique_id = unique_id_disciplinary_public)

# This is our datasets being joined into one
dataset <- data1 |> left_join(data2) |>
  left_join(data3) |>
  left_join(data4) |>
  left_join(data5) |>
  left_join(data6) |>
  left_join(data7) |>
  left_join(data8) |>
  left_join(data9) |>
  left_join(data10) |>
  left_join(data11) |>
  left_join(data12) 
```



## Removing Useless Columns

```{r remove cols}

# # # # # # # # # # # # # #
## Removing Useless Columns

#remove NAs
dataset[is.na(dataset)] <- 0

#remove repeated columns (like unitid repeating for each xcel file)
#(3/4/24) just fixed some problems w this

cols_to_remove <- c("Unitid_student_housing", "Institution.name_student_housing", "OPEID_student_housing", "Campus.ID_student_housing", "Campus.Name_student_housing", "Institution.Size_student_housing", "Unitid_crim_offense_noncampus", "Institution.name_crim_offense_noncampus", "OPEID_crim_offense_noncampus", "Campus.ID_crim_offense_noncampus", "Campus.Name_crim_offense_noncampus", "Institution.Size_crim_offense_noncampus", "Unitid_crim_offense_public", "Institution.name_crim_offense_public", "OPEID_crim_offense_public", "Campus.ID_crim_offense_public", "Campus.Name_crim_offense_public", "Institution.Size_crim_offense_public", "Unitid_arrests_campus", "Institution.name_arrests_campus", "OPEID_arrests_campus", "Campus.ID_arrests_campus", "Campus.Name_arrests_campus", "Institution.Size_arrests_campus", "Unitid_arrests_stuhousing", "Institution.name_arrests_stuhousing", "OPEID_arrests_stuhousing", "Campus.ID_arrests_stuhousing", "Campus.Name_arrests_stuhousing", "Institution.Size_arrests_stuhousing", "Unitid_arrests_noncampus", "Institution.name_arrests_noncampus", "OPEID_arrests_noncampus", "Campus.ID_arrests_noncampus", "Campus.Name_arrests_noncampus", "Institution.Size_arrests_noncampus", "Unitid_arrests_public", "Institution.name_arrests_public", "OPEID_arrests_public", "Campus.ID_arrests_public", "Campus.Name_arrests_public", "Institution.Size_arrests_public", "Unitid_disciplinary_campus", "Institution.name_disciplinary_campus", "OPEID_disciplinary_campus", "Campus.ID_disciplinary_campus", "Campus.Name_disciplinary_campus", "Institution.Size_disciplinary_campus", "Unitid_disciplinary_noncampus", "Institution.name_disciplinary_noncampus", "OPEID_disciplinary_noncampus", "Campus.ID_disciplinary_noncampus", "Campus.Name_disciplinary_noncampus", "Institution.Size_disciplinary_noncampus", "Unitid_disciplinary_public", "Institution.name_disciplinary_public", "OPEID_disciplinary_public", "Campus.ID_disciplinary_public", "Campus.Name_disciplinary_public", "Institution.Size_disciplinary_public", "Unitid_disciplinary_housing", "Institution.name_disciplinary_housing", "OPEID_disciplinary_housing", "Campus.ID_disciplinary_housing", "Campus.Name_disciplinary_housing", "Institution.Size_disciplinary_housing")


## had to change this dataset name before removing the campses ##

cleaned <- dataset[, !names(dataset) %in% cols_to_remove]
```

## Remove Campuses

Removes campuses outside of Colorado.

```{r remove campuses}

# # # # # # # # # # # # # # # # #
## Remove Campuses Outside of CO

# had to split into 3 vectors otherwise it's too long
to_remove1 <- c("Jacksonville", "San Diego", "Memphis", "Dunnam", "Ft. Drum", "San Luis Obispo", "Syracuse", "Whidbey", "Marysville", "Crystal Lake", "Waynesville", "Ft. Still", "Patrick AFB", "Los Alamitos", "Rolla", "Jefferson City", "Navy College Office", "Ft. Leonard Wood", "Coast Guard Island", "NAS Ft Worth JRB", "Lake County", "Lake of the Ozarks", "NS Great Lakes", "Hunter Army", "Orlando", "Amarillo Texas", "Sky Harbor", "Fort Benning", "Greenville", "Kaneohe", "Pheonix-Mesa", "Patuxent River", "El Paso", "Langley", "Shaw", "Sheppard", "Mildenhall", "Spangdahlem", "Pensacola", "Minot", "San Antonio", "Fort Eustis", "Europe", "Dyess", "Beaufort", "Los Angeles", "Fort Walton Beach", "Rota", "Mobile", "Huntsville", "Everett", "MacDill", "North Island", "Houston", "Charleston", "Ghana", "Atlanta", "Fort Jackson", "Corpus Christi", "Honolulu", "St. Petersburg", "Tampa Bay", "Anchorage", "Seattle", "Camp Pendleton", "Space Coast Perimeter", "Fort Huachuca", "Jefferson City", "Inland Empire", "McConnell", "Lemoore", "Crestview", "New Orleans", "Camp Humphreys", "Fort Leavenworth", "Chicago", "Little Rock", "Geilenkirchen", "Fort Lauderdale", "Davis-Monthan", "Ramstein", "Mountain Home", "Connecticut", "Kadena", "Tyndall", "Philadelphia", "Barksdale", "Travis", "Ft Worth", "Leiden", "Clearfield", "Luke", "Miami", "Fort Bliss", "Tacoma", "Lakeland", "North Charleston", "Oceanside", "Myrtle Beach", "Fort Bragg", "Yokosuka", "Northwest Kansas", "Atsugi", "DFW", "Futenma", "Offutt", "Ventura", "Aviano", "Clovis", "Kansas City", "Las Vegas", "Tinker Air Force Base", "Greenville Metropolitan Campus", "Robins", "Riverside Airport")

#check vector length
#length(to_remove1)

# check to see if campus name is in there
matches <- unique(grep(paste(to_remove1,collapse="|"), 
                        cleaned$Campus.Name_all_campus, value=TRUE))
# new dataset which excludes campuses from remove_1
cleaned_1 <- cleaned |> filter(!Campus.Name_all_campus %in% matches)

# same process as above
to_remove2 <- c("Albuquerque", "Wiesbaden", "Beale", "Gateway", "Ocala Metropolitan Campus", "Baton Rouge", "Schofield Barracks", "Portland", "Hartford", "Hunter Airfield Education", "Hurlburt Field", "Los Angeles Air Force Base", "Savannah Area", "Columbia College (Main Campus) @ Columbia", "Columbia College @ NS Great Lakes", "Lakenheath", "Sigonella", "Spokane", "Yokota", "Northern Utah", "Louisville", "Norfolk", "Fort Gordon", " Shaw Air Force Base", "Andrews Air Force Base", "Fort Belvoir", "Greensboro", "Moody", "China Lake", "McConnell Air Force Base", "McGuire", "Ft. Sill", "Geneva", "Fairchild Air Force Base", "Wesport Plaza", "NS Mayport", "Andrews", "St. Louis", "Fort Smith", "Melbourne", "Lackland Air Force base", "Phoenix-Mesa", "Fort Worth", "Afghanistan", "Rockford", "Katterbach", "Vienna", "Ellsworth", "Mather", "Webster University at Elgin", "Great Lakes Naval", "Trinidad Campus", "Cheyenne Campus", "Great Falls", "Fort Sill", "Moberly Area Community College", "Edwards Air Force Base", "Asbury Theological Seminary", "Fort Campbell", "Lincoln College of Technology", "Winghaven Campus", "Cha-Am/Hua-Hin", "Freeport", "Tallahassee", "Patrick Air Force Base", "Ft. Stewart", "San Francisco", "Scott Air Force Base", "Camp Lejeune", "Fallon", "Altus", "Indianapolis", "Oceana", "Northwest Arkansas (Rogers) Metropolitan Campus", "Seymour Johnson", "Cheyenne", "Kuwait", "Victorville", "Salt Lake City", "Incirlik", "Whiteman Air Force Base", "Tucson", "Tampa", "Elgin", "Oklahoma City", "Elizabeth City", "Dayton Area", "Columbia College @ St. Louis", "Palmdale", "Lajes Field", "Sand Island - HI", "Merrit Island (Space Coast) Metropolitan Campus", "Fort Sill", "Vance", "Westport Plaza", "Lackland Air Force Base", "Columbia", "Coast Guard", "Merrit Island", "Vandenberg AFB")

#length(to_remove2)

matches <- unique(grep(paste(to_remove2,collapse="|"), 
                        cleaned_1$Campus.Name_all_campus, value=TRUE))
cleaned_2 <- cleaned_1 |> filter(!Campus.Name_all_campus %in% matches)

to_remove3 <- c("Webster University St. Louis-Main Campus", "Space Coast", "Fort Worth", "San Francisco", "Cincinnati", "Oceana", "Holloman", "Columbia Metropolitan Campus", "Melbourne Campus", "Columbus AFB- MS", "Rockford", "Columbia College @ Ft. Sill", "Schriever", "Great Lakes Naval", "Keesler", "Columbia College @ Ft. Stewart", "Coast Guard Air Station (Clearwater)", "Camp Smith", "Whiteman Air Force Base", "Sarasota Metropolitan Campus", "Barbers Point- HI", "Vendenberg AFB", "Whiting Field", "Geneva", "Sand Island- HI", "Concorde Career College", "Irvine Metropolitan", "Valley Campus", "Hill Air Force Base", "Redstone Arsenal", "Laughlin", "NS Guantanamo Bay", "Fort Sill", "Ozark Metropolitan Campus", "NS Mayport", " Fort Rucker", "New River (MCAS)", "Landover Site", "Columbia College @ Elgin", "International Salon and Spa Academy", "Nalanda Campus", "Colubia College @Mesquite", "Great Lakes Naval", "Delaware", "Columbia Metropolitan Campus", "Quantico", "Columbia College @ Salt Lake City", "Bolling Air Force Base", "Mesquite", "Dallas/Fort Worth", "Osan", "Randolph Air Force Base", "Newark Campus", "Springfield", "Southern Maryland Higher Education Center (SMHEC)", "Fort Leonard Wood", "Columbia College Kings Bay", "Vienna", "Freeport", "Wright Patterson", "Irvine", "Columbia College (Main Campus) @ Columbia", "Ft. Stewart", "Washington- D.C.", "Joint Base Myer/Henderson Hall", "Tulsa", "Bangkok", "Nashville", "Hohenfels", "Athens", "New River", "Misawa", "Columbia College Imperial", "PCC Bayfield Site", "Iwanuki", "Asbury Theological Seminary - Tulsa - OK Instructional Site", "Grefenwoehr", "Webster University St. Louis-Main Campus", "Vilseck", "Arkansas", "New River", "Maryland")

#length(to_remove3)

matches <- unique(grep(paste(to_remove3,collapse="|"), 
                        cleaned_2$Campus.Name_all_campus, value=TRUE))

# final cleaned dataset
cleaned_data <- cleaned_2 |> filter(!Campus.Name_all_campus %in% matches)


# take a look
#head(cleaned_data)

```


# Summary Statistics and EDA

```{r sum stat 1}

# # # # # # # # # # # # # ##
# SUMMARY STATISTICS & EDA #
# # # # # # # # # # # # # ##

#new column combining liquor law violations across disciplinary, arrests and location (public, stuhousing, campus, noncampus)
cleaned_data$all_liquor_violations <- cleaned_data$Liquor.law.violations_arrests_campus + cleaned_data$Liquor.law.violations_arrests_noncampus + cleaned_data$Liquor.law.violations_arrests_public + cleaned_data$Liquor.law.violations_arrests_stuhousing + cleaned_data$Liquor.law.violations_disciplinary_campus + cleaned_data$Liquor.law.violations_disciplinary_housing + cleaned_data$Liquor.law.violations_disciplinary_noncampus + cleaned_data$Liquor.law.violations_disciplinary_public

numeric_data <- select(cleaned_data, where(is.numeric))

# # # # # # # # # # # # # #
## Institution Size v. TLV

institution_size <- ifelse(cleaned_data$Institution.Size_all_campus > 15000, "Large", "Small")

ggplot(cleaned_data, aes(x = institution_size, y = all_liquor_violations, fill = institution_size)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(title = "Comparison of Institution Size With Mean Liquor Law Violations",
       x = "Institution Size",
       y = "Mean Liquor Violations") +
  scale_fill_manual(values = c("Large" = "skyblue", "Small" = "darkblue")) +  # Set custom fill colors
  theme_minimal() +
  theme(legend.position = "none")  # Remove legend

# # # # # # # # # # # # # # # # # # # #
## Plot of Colleges w/ 1000+ Violations

cleaned_data |> filter(all_liquor_violations > 1000) |>
 ggplot() +
  geom_point(aes(x = Institution.name_all_campus, y = all_liquor_violations),
             color = "black") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Liquor Law Violations Per College Campus (Schools with 1000+ Violations)") +
  xlab("Institution") + ylab("Liquor Violations") 


# # # # # # # # # # # # # # # # # # # # # # #
## Kable Table for Means and SD of variables

# Sample data creation (assuming 'cleaned' is your data frame)
means <- round(c(mean(cleaned_data$Negligent.manslaughter_all_campus),
                 mean(cleaned_data$Sex.offenses...Forcible_all_campus),
                 mean(cleaned_data$Rape_all_campus),
                 mean(cleaned_data$Fondling_all_campus),
                 mean(cleaned_data$Sex.offenses...Non.forcible_all_campus),
                 mean(cleaned_data$Incest_all_campus),
                 mean(cleaned_data$Statutory.rape_all_campus),
                 mean(cleaned_data$Robbery_all_campus),
                 mean(cleaned_data$Burglary_all_campus),
                 mean(cleaned_data$Motor.vehicle.theft_all_campus),
                 mean(cleaned_data$Arson_all_campus)), 3)

sds <- round(c(
  sd(cleaned_data$Negligent.manslaughter_all_campus),
  sd(cleaned_data$Sex.offenses...Forcible_all_campus),
  sd(cleaned_data$Rape_all_campus),
  sd(cleaned_data$Fondling_all_campus),
  sd(cleaned_data$Sex.offenses...Non.forcible_all_campus),
  sd(cleaned_data$Incest_all_campus),
  sd(cleaned_data$Statutory.rape_all_campus),
  sd(cleaned_data$Robbery_all_campus),
  sd(cleaned_data$Burglary_all_campus),
  sd(cleaned_data$Motor.vehicle.theft_all_campus),
  sd(cleaned_data$Arson_all_campus)
), 3)

# Creating data frame
summary_df <- data.frame(
  Variable  = c("Negligent Manslaughter", "Sex Offenses (Forcible)", "Rape",
                "Fondling", "Sex Offenses (Non-forcible)", "Incest",
                "Statutory Rape", "Robbery", "Burglary", "Motor Vehicle Theft",
                "Arson"),
  Mean = means,
  StandardDeviation = sds
)

# Sorting the data frame by Mean in descending order
sorted_summary_df <- summary_df %>%
  arrange(desc(Mean), desc(StandardDeviation))

# Creating the kable
knitr::kable(sorted_summary_df, caption = "Average Values of Different Campus Offenses",
             col.names = c("Variables", "Average", "Standard Deviation"))


# # # # # # # # # # # # # #
## Barplot of TLV vs. Year

year_factor <- as.factor(cleaned_data$Survey.year)

ggplot(cleaned_data, aes(x = year_factor, y = all_liquor_violations, fill = year_factor)) +
  geom_bar(stat = "identity") +
  labs(x = "Year", y = "Liquor Law Violations", fill = "Year") +
  ggtitle("Barplot of Total Liquor Violations vs. Year") +
  theme(legend.position = "none")

```

# split data

```{r}
set.seed(4242)

## split cleaned data into 25/75
smp_size <- floor(0.75 * nrow(cleaned_data))

train_split <- sample(seq_len(nrow(cleaned_data)), size = smp_size)

# create train = 75% and test = 25% set
train <- cleaned_data[train_split,] |> as_tibble() |> mutate(train = TRUE)
test <- cleaned_data[-train_split,] |> as_tibble() |> mutate(train = FALSE)


## check split to ensure nothing got screwed up

# create df of training data means and sd of each column
train_means_sd <- sapply(train[,c(7:20, 22:86)], 
                         function(x) c(mean(x, na.rm = TRUE), 
                                       sd(x, na.rm=TRUE)),
                         simplify = FALSE) |> bind_rows()
# transpose so table is legible
ttrain_means_sd <- t(train_means_sd)
# create kable table
#knitr::kable(ttrain_means_sd, digits = 5, caption = "Training Data, metrics to compare to test", col.names = c("Columns", "Mean", "SD"))

# create df of testing data means and sd of each column
test_means_sd <- sapply(test[,c(7:20, 22:86)], 
                         function(x) c(mean(x, na.rm = TRUE), 
                                       sd(x, na.rm=TRUE)),
                         simplify = FALSE) |> bind_rows()
ttest_means_sd <- t(test_means_sd)
#knitr::kable(ttest_means_sd, digits = 5, caption = "Test Data, metrics to compare to training", col.names = c("Columns", "Mean", "SD"))


## kable tables for hw 5

train_means <- round(c(mean(train$Negligent.manslaughter_all_campus),
           mean(train$Sex.offenses...Forcible_all_campus),
           mean(train$Rape_all_campus),
           mean(train$Fondling_all_campus),
           mean(train$Sex.offenses...Non.forcible_all_campus),
           mean(train$Incest_all_campus),
           mean(train$Statutory.rape_all_campus),
           mean(train$Robbery_all_campus),
           mean(train$Burglary_all_campus),
           mean(train$Motor.vehicle.theft_all_campus),
           mean(train$Arson_all_campus)), 3)

train_sds <- round(c(
  sd(train$Negligent.manslaughter_all_campus),
  sd(train$Sex.offenses...Forcible_all_campus),
  sd(train$Rape_all_campus),
  sd(train$Fondling_all_campus),
  sd(train$Sex.offenses...Non.forcible_all_campus),
  sd(train$Incest_all_campus),
  sd(train$Statutory.rape_all_campus),
  sd(train$Robbery_all_campus),
  sd(train$Burglary_all_campus),
  sd(train$Motor.vehicle.theft_all_campus),
  sd(train$Arson_all_campus)
), 3)

train_pres <- data.frame(
  Variable  = c("Negligent Manslaughter", "Sex Offenses (Forcible)", "Rape",
               "Fondling", "Sex Offenses (Non-forcible)", "Incest",
               "Statutory Rape", "Robbery", "Burglary", "Motor Vehicle Theft",
               "Arson"),
  Mean = train_means,
  StandardDeviation = train_sds
)

knitr::kable(train_pres, caption = "Training Data", col.names = c("Variable", "Mean", "SD"))

test_means <- round(c(mean(test$Negligent.manslaughter_all_campus),
           mean(test$Sex.offenses...Forcible_all_campus),
           mean(test$Rape_all_campus),
           mean(test$Fondling_all_campus),
           mean(test$Sex.offenses...Non.forcible_all_campus),
           mean(test$Incest_all_campus),
           mean(test$Statutory.rape_all_campus),
           mean(test$Robbery_all_campus),
           mean(test$Burglary_all_campus),
           mean(test$Motor.vehicle.theft_all_campus),
           mean(test$Arson_all_campus)), 3)

test_sds <- round(c(
  sd(test$Negligent.manslaughter_all_campus),
  sd(test$Sex.offenses...Forcible_all_campus),
  sd(test$Rape_all_campus),
  sd(test$Fondling_all_campus),
  sd(test$Sex.offenses...Non.forcible_all_campus),
  sd(test$Incest_all_campus),
  sd(test$Statutory.rape_all_campus),
  sd(test$Robbery_all_campus),
  sd(test$Burglary_all_campus),
  sd(test$Motor.vehicle.theft_all_campus),
  sd(test$Arson_all_campus)
), 3)

test_pres <- data.frame(
  Variable  = c("Negligent Manslaughter", "Sex Offenses (Forcible)", "Rape",
               "Fondling", "Sex Offenses (Non-forcible)", "Incest",
               "Statutory Rape", "Robbery", "Burglary", "Motor Vehicle Theft",
               "Arson"),
  Mean = test_means,
  StandardDeviation = test_sds
)

knitr::kable(test_pres, caption = "Test Data", col.names = c("Variable", "Mean", "SD"))

```

# XGBoost

```{r XGBoost}

# # # # # #
# XGBOOST #
# # # # # #

# # # # # # # # # # # # # # # # # #
## Find Predictors Relevant to Data

# Ensure train is a data frame
if (!is.data.frame(train)) {
  stop("train must be a data frame.")
}

# Exclude specified columns
excluded_columns <- c("Survey.year", "Unitid_all_campus", 
                      "OPEID_all_campus", "Campus.ID_all_campus", "Campus.Name_all_campus", 
                      "Institution.Size_all_campus")

# Select only numeric columns (excluding the excluded columns)
numeric_train <- train[, sapply(train, is.numeric) & 
                       !(names(train) %in% excluded_columns)]

# Calculate the sum of each numeric column
column_sums <- colSums(numeric_train, na.rm = TRUE)

# Sort the column sums from most to least
sorted_column_sums <- sort(column_sums, decreasing = TRUE)

# Print the sorted column sums
print(sorted_column_sums)

# # # # # # # # # # #
## Correlation Tests

# Check the data types of the columns
class(train$Arson_crim_offense_noncampus)
class(train$all_liquor_violations)

# If any of the columns are not numeric, convert them to numeric
train$Arson_crim_offense_noncampus <- as.numeric(train$Arson_crim_offense_noncampus)
train$all_liquor_violations <- as.numeric(train$all_liquor_violations)

# Perform Pearson's correlation test
correlation_result <- cor.test(train$Arson_crim_offense_noncampus, train$all_liquor_violations)

# Format the correlation test result
formatted_result <- sprintf("Correlation test result:
  - t-value = %.3f
  - degrees of freedom = %d
  - p-value = %s
  - correlation estimate = %.3f
  - 95 percent confidence interval: [%.3f, %.3f]",
  correlation_result$statistic,
  correlation_result$parameter,
  format(correlation_result$p.value),
  correlation_result$estimate,
  correlation_result$conf.int[1],
  correlation_result$conf.int[2]
)

# Print the formatted result
cat(formatted_result)

# # # # # # # # # #
## Developing Model

# Using variables with a positive linear relationship with "all_liquor_violations"
# Define the selected variables for modeling
selected_vars <- c(
  "Liquor.law.violations_disciplinary_campus",
  "Liquor.law.violations_disciplinary_housing",
  "Drug.law.violations_disciplinary_campus",
  "Drug.law.violations_disciplinary_housing",
  "Liquor.law.violations_arrests_campus",
  "Drug.law.violations_arrests_campus",
  "Liquor.law.violations_arrests_stuhousing",
  "Drug.law.violations_arrests_stuhousing",
  "Burglary_all_campus",
  "Liquor.law.violations_disciplinary_noncampus",
  "Drug.law.violations_arrests_public",
  "Liquor.law.violations_arrests_public",
  "Motor.vehicle.theft_all_campus",
  "Burglary_student_housing",
  "Rape_all_campus",
  "Drug.law.violations_disciplinary_noncampus",
  "Rape_student_housing",
  "Liquor.law.violations_disciplinary_public",
  "Fondling_all_campus",
  "Aggravated.assault_all_campus",
  "Liquor.law.violations_arrests_noncampus",
  "Drug.law.violations_arrests_noncampus",
  "Motor.vehicle.theft_crim_offense_public",
  "Robbery_crim_offense_public",
  "Illegal.weapons.possession_arrests_campus",
  "Aggravated.assault_crim_offense_public",
  "Fondling_student_housing",
  "Sex.offenses...Forcible_all_campus",
  "Illegal.weapons.possession_disciplinary_campus",
  "Illegal.weapons.possession_arrests_public",
  "Robbery_all_campus",
  "Sex.offenses...Forcible_student_housing",
  "Arson_all_campus",
  "Illegal.weapons.possession_disciplinary_housing",
  "Burglary_crim_offense_noncampus",
  "Drug.law.violations_disciplinary_public",
  "Aggravated.assault_student_housing",
  "Arson_student_housing",
  "Motor.vehicle.theft_crim_offense_noncampus",
  "Fondling_crim_offense_public",
  "Sex.offenses...Forcible_crim_offense_public",
  "Aggravated.assault_crim_offense_noncampus",
  "Rape_crim_offense_noncampus",
  "Fondling_crim_offense_noncampus",
  "Illegal.weapons.possession_arrests_stuhousing",
  "Sex.offenses...Forcible_crim_offense_noncampus",
  "Rape_crim_offense_public",
  "Arson_crim_offense_public",
  "Robbery_crim_offense_noncampus",
  "Illegal.weapons.possession_arrests_noncampus",
  "Arson_crim_offense_noncampus"
)

# Subset the data with selected variables
# This step extracts only the columns from the dataset that are relevant for the analysis or modeling task.
# It filters out unnecessary or redundant features, focusing the analysis on the variables thought to have the most impact on the target variable.
# This helps simplify the dataset, improves model interpretability, and potentially enhances model performance by reducing noise and overfitting.
train_subset <- train[selected_vars]

# Extract the target variable
# This step isolates the target variable "all_liquor_violations".
# "all_liquor_violations" represents the outcome that the model learns to predict based on the input features.
# By separating the target variable, the modeling process can focus on understanding the relationships between the input features and the target outcome.
y_train <- train$all_liquor_violations

# Train an ensemble model using XGBoost
# This step builds a predictive model using the XGBoost algorithm.
# The model is trained on the selected variables and the target variable.
# Hyperparameters like nrounds and verbose are specified to control the training process.
# Higher values for nrounds allow the model to learn more complex patterns in the data but may increase the risk of overfitting.
xgb_model <- xgboost(data = as.matrix(train_subset), 
                     label = y_train, 
                     nrounds = 100, 
                     verbose = 0)
xgb_validation1 <- xgb.cv(data = as.matrix(train_subset), 
                     label = y_train, 
                     nrounds = 100, 
                     nfold = 10,
                     verbose = 0)
# do 5 for nrounds

# # # # # # # # # # #
## Plot for N-Rounds

# Print the trained model
print("Trained XGBoost Model:")
print(xgb_model)
print(xgb_validation1)

eval_metrics <- xgb_validation1$evaluation_log
plot(eval_metrics$iter, eval_metrics$train_rmse_mean, 
     xlim = c(0,20),
     xlab = "Iterations",
     ylab = "Mean RMSE",
     main = "Mean RMSE vs. Iterations Interaction") # train
points(eval_metrics$iter, eval_metrics$test_rmse_mean, col = "red") # test
legend("topright", legend = c("Training RMSE", "Test RMSE"),
       col = c("black", "red"),
       pch = c(1,1),
       cex = 0.8,
       bg = "white",
       box.lty = 0)

# # # # # # # # # # #
## Model Evaluation

xgb_model <- xgboost(data = as.matrix(train_subset), 
                     label = y_train, 
                     nrounds = 5, 
                     verbose = 0)

test_subset <- test[selected_vars]
test_predictions <- predict(xgb_model, as.matrix(test_subset))
test_rmse <- sqrt(mean((test_predictions - test$all_liquor_violations)^2))
cat("Test RMSE:", test_rmse, "\n")

```



# K-Means Clustering

```{r}

# # # # # # # # # # ##
# K-MEANS CLUSTERING #
# # # # # # # # # # ##

alc_2cols1 <- train_num[ , c("all_liquor_violations", "Institution.Size_all_campus")]
set.seed(421)
km.out <- kmeans(alc_2cols1, centers = 3, nstart = 20)
km.out

# # # # # # # # # # # # # # # # # # # #
## Finding Optimal Number of Clusters

nclust <- 10
wss <- numeric(nclust)
set.seed(421)
## Looping through different number of clusters 
for (i in 1:nclust) {
  km.out <- kmeans(alc_2cols1, centers = i, nstart = 20)
  wss[i] <- km.out$tot.withinss
}

## Plotting 
wss_df <- tibble(clusters = 1:nclust, wss = wss)
sc_plot <- ggplot(wss_df, aes(x = clusters, y = wss, group = 1)) +
  geom_point(size = 3) + 
  geom_line() +
  scale_x_continuous(breaks = c(2, 4, 6, 8, 10)) +
  scale_y_continuous(labels = scales::comma) +
  xlab("Number of Clusters") +
  ylab("Within Cluster Sum of Squares") +
  ggtitle("Optimal Number of Clusters, Scree Plot")
sc_plot +
  geom_hline(
    yintercept = wss,
    linetype = 'dashed',
    col = c(rep('black',4),'red', rep('black', 5))
  )

# # # # # # # # # # # # # # # # # # # #
## Plotting Optimal Number of Clusters

k <- 5

set.seed(421)
km.out <- kmeans(alc_2cols1, centers = k, nstart = 20)

train_num$cluster_id <- factor(km.out$cluster)
ggplot(train_num, aes(Institution.Size_all_campus, all_liquor_violations, color = cluster_id)) +
    geom_point(alpha = 0.40) +
    xlab("Institution Size (Number of Students Enrolled)") +
    ylab("Liquor Violations") +
  ggtitle("Plot of Clustered Liquor Law Violations by Institution Size") +
  labs(color = "Cluster #")



idx <- which(km.out$cluster == 5)
train[idx,]$Institution.name_all_campus


idx <- km.out$cluster
idx <- which(km.out$cluster == 5)
train[idx,]
train[idx,]$Institution.name_all_campus


# # # # # # # # # # # # # # # # #
## Clustering with 2 Predictors

test_num <- test |> as_tibble() |> select(-where(is.character))
test_num <- test_num[, !names(test_num) %in% c('Unitid_all_campus', 'OPEID_all_campus', 'Campus.ID_all_campus')]
cols_test <- train_num[ , c("all_liquor_violations", "Survey.year", "Institution.Size_all_campus")]

kmTEST <- kmeans(cols_test, centers = k, nstart = 20)

sil <- silhouette(kmTEST$cluster, dist(train_num))

data_frame <- data.frame(sil_width = sil[, "sil_width"],
                         cluster = sil[, "cluster"])
avg_sil_scores_by_cluster <- data_frame %>%
  group_by(cluster) %>%
  summarise(avg_silhouette = mean(sil_width))
print(avg_sil_scores_by_cluster)
kable(avg_sil_scores_by_cluster, format = "markdown", 
      col.names =c("Cluster", "Average Silhouette Score"),
        caption = "Average Silhouette Scores by Cluster")

# # # # # #
## 3D Plot

#library(plotly)

#fig <- plot_ly(train_num, x = ~Institution.Size_all_campus, y = ~Survey.year, z = ~all_liquor_violations, type = 'scatter3d', mode = 'markers', color = ~cluster_id2)
#fig <- fig %>% layout(title = "Cluster Plot of Institution Size and Survey Year",
          #            scene = list(xaxis = list(title = "Institution Size"),
           #                        yaxis = list(title = "Survey Year"),
            #                       zaxis = list(title = "Liquor Law Violations")))
#fig
```


# Neural Network

### plot.nnet
```{r Neural Network}

# # # # # # # # ##
# NEURAL NETWORK #
# # # # # # # # ##

# # # # # # # # # # # # # # # # # # # #
## Plot Function for Clean NN Plotting

## for plot.nnet()
#install.packages("devtools")
library(devtools)
#install.packages("reshape")
library(reshape)

# import plot nnet function from github
plot.nnet <- function(mod.in,nid=T,all.out=T,all.in=T,bias=T,wts.only=F,rel.rsc=5,circle.cex=5,
                    node.labs=T,var.labs=T,x.lab=NULL,y.lab=NULL,line.stag=NULL,struct=NULL,cex.val=1,
                    alpha.val=1,circle.col='lightblue',pos.col='black',neg.col='grey', max.sp = F, ...){
  
  require(scales)
  
  #sanity checks
  if('mlp' %in% class(mod.in)) warning('Bias layer not applicable for rsnns object')
  if('numeric' %in% class(mod.in)){
    if(is.null(struct)) stop('Three-element vector required for struct')
    if(length(mod.in) != ((struct[1]*struct[2]+struct[2]*struct[3])+(struct[3]+struct[2])))
      stop('Incorrect length of weight matrix for given network structure')
  }
  if('train' %in% class(mod.in)){
    if('nnet' %in% class(mod.in$finalModel)){
      mod.in<-mod.in$finalModel
      warning('Using best nnet model from train output')
    }
    else stop('Only nnet method can be used with train object')
  }
  
  #gets weights for neural network, output is list
  #if rescaled argument is true, weights are returned but rescaled based on abs value
  nnet.vals<-function(mod.in,nid,rel.rsc,struct.out=struct){
    
    require(scales)
    require(reshape)
    
    if('numeric' %in% class(mod.in)){
      struct.out<-struct
      wts<-mod.in
    }
    
    #neuralnet package
    if('nn' %in% class(mod.in)){
      struct.out<-unlist(lapply(mod.in$weights[[1]],ncol))
    	struct.out<-struct.out[-length(struct.out)]
    	struct.out<-c(
    		length(mod.in$model.list$variables),
    		struct.out,
    		length(mod.in$model.list$response)
    		)    		
      wts<-unlist(mod.in$weights[[1]])   
    }
    
    #nnet package
    if('nnet' %in% class(mod.in)){
      struct.out<-mod.in$n
      wts<-mod.in$wts
    }
    
    #RSNNS package
    if('mlp' %in% class(mod.in)){
      struct.out<-c(mod.in$nInputs,mod.in$archParams$size,mod.in$nOutputs)
      hid.num<-length(struct.out)-2
      wts<-mod.in$snnsObject$getCompleteWeightMatrix()
      
      #get all input-hidden and hidden-hidden wts
      inps<-wts[grep('Input',row.names(wts)),grep('Hidden_2',colnames(wts)),drop=F]
      inps<-melt(rbind(rep(NA,ncol(inps)),inps))$value
      uni.hids<-paste0('Hidden_',1+seq(1,hid.num))
      for(i in 1:length(uni.hids)){
        if(is.na(uni.hids[i+1])) break
        tmp<-wts[grep(uni.hids[i],rownames(wts)),grep(uni.hids[i+1],colnames(wts)),drop=F]
        inps<-c(inps,melt(rbind(rep(NA,ncol(tmp)),tmp))$value)
        }
      
      #get connections from last hidden to output layers
      outs<-wts[grep(paste0('Hidden_',hid.num+1),row.names(wts)),grep('Output',colnames(wts)),drop=F]
      outs<-rbind(rep(NA,ncol(outs)),outs)
      
      #weight vector for all
      wts<-c(inps,melt(outs)$value)
      assign('bias',F,envir=environment(nnet.vals))
      }
    
    if(nid) wts<-rescale(abs(wts),c(1,rel.rsc))
    
    #convert wts to list with appropriate names 
    hid.struct<-struct.out[-c(length(struct.out))]
    row.nms<-NULL
    for(i in 1:length(hid.struct)){
      if(is.na(hid.struct[i+1])) break
      row.nms<-c(row.nms,rep(paste('hidden',i,seq(1:hid.struct[i+1])),each=1+hid.struct[i]))
    }
    row.nms<-c(
      row.nms,
      rep(paste('out',seq(1:struct.out[length(struct.out)])),each=1+struct.out[length(struct.out)-1])
      )
    out.ls<-data.frame(wts,row.nms)
    out.ls$row.nms<-factor(row.nms,levels=unique(row.nms),labels=unique(row.nms))
    out.ls<-split(out.ls$wts,f=out.ls$row.nms)
    
    assign('struct',struct.out,envir=environment(nnet.vals))
    
    out.ls
    
    }
  
  wts<-nnet.vals(mod.in,nid=F)
  
  if(wts.only) return(wts)
  
  #circle colors for input, if desired, must be two-vector list, first vector is for input layer
  if(is.list(circle.col)){
                    circle.col.inp<-circle.col[[1]]
                    circle.col<-circle.col[[2]]
                    }
  else circle.col.inp<-circle.col
  
  #initiate plotting
  x.range<-c(0,100)
  y.range<-c(0,100)
  #these are all proportions from 0-1
  if(is.null(line.stag)) line.stag<-0.011*circle.cex/2
  layer.x<-seq(0.17,0.9,length=length(struct))
  bias.x<-layer.x[-length(layer.x)]+diff(layer.x)/2
  bias.y<-0.95
  circle.cex<-circle.cex
  
  #get variable names from mod.in object
  #change to user input if supplied
  if('numeric' %in% class(mod.in)){
    x.names<-paste0(rep('X',struct[1]),seq(1:struct[1]))
    y.names<-paste0(rep('Y',struct[3]),seq(1:struct[3]))
  }
  if('mlp' %in% class(mod.in)){
    all.names<-mod.in$snnsObject$getUnitDefinitions()
    x.names<-all.names[grep('Input',all.names$unitName),'unitName']
    y.names<-all.names[grep('Output',all.names$unitName),'unitName']
  }
  if('nn' %in% class(mod.in)){
    x.names<-mod.in$model.list$variables
    y.names<-mod.in$model.list$respons
  }
  if('xNames' %in% names(mod.in)){
    x.names<-mod.in$xNames
    y.names<-attr(terms(mod.in),'factor')
    y.names<-row.names(y.names)[!row.names(y.names) %in% x.names]
  }
  if(!'xNames' %in% names(mod.in) & 'nnet' %in% class(mod.in)){
    if(is.null(mod.in$call$formula)){
      x.names<-colnames(eval(mod.in$call$x))
      y.names<-colnames(eval(mod.in$call$y))
    }
    else{
      forms<-eval(mod.in$call$formula)
      x.names<-mod.in$coefnames
      facts<-attr(terms(mod.in),'factors')
      y.check<-mod.in$fitted
      if(ncol(y.check)>1) y.names<-colnames(y.check)
      else y.names<-as.character(forms)[2]
    } 
  }
  #change variables names to user sub 
  if(!is.null(x.lab)){
    if(length(x.names) != length(x.lab)) stop('x.lab length not equal to number of input variables')
    else x.names<-x.lab
  }
  if(!is.null(y.lab)){
    if(length(y.names) != length(y.lab)) stop('y.lab length not equal to number of output variables')
    else y.names<-y.lab
  }
  
  #initiate plot
  plot(x.range,y.range,type='n',axes=F,ylab='',xlab='',...)
  
  #function for getting y locations for input, hidden, output layers
  #input is integer value from 'struct'
  get.ys<-function(lyr, max_space = max.sp){
  	if(max_space){ 
  		spacing <- diff(c(0*diff(y.range),0.9*diff(y.range)))/lyr
   	} else {
    	spacing<-diff(c(0*diff(y.range),0.9*diff(y.range)))/max(struct)
   	}
    
  		seq(0.5*(diff(y.range)+spacing*(lyr-1)),0.5*(diff(y.range)-spacing*(lyr-1)),
        length=lyr)
  }
  
  #function for plotting nodes
  #'layer' specifies which layer, integer from 'struct'
  #'x.loc' indicates x location for layer, integer from 'layer.x'
  #'layer.name' is string indicating text to put in node
  layer.points<-function(layer,x.loc,layer.name,cex=cex.val){
    x<-rep(x.loc*diff(x.range),layer)
    y<-get.ys(layer)
    points(x,y,pch=21,cex=circle.cex,col=in.col,bg=bord.col)
    if(node.labs) text(x,y,paste(layer.name,1:layer,sep=''),cex=cex.val)
    if(layer.name=='I' & var.labs) text(x-line.stag*diff(x.range),y,x.names,pos=2,cex=cex.val)      
    if(layer.name=='O' & var.labs) text(x+line.stag*diff(x.range),y,y.names,pos=4,cex=cex.val)
  }
  
  #function for plotting bias points
  #'bias.x' is vector of values for x locations
  #'bias.y' is vector for y location
  #'layer.name' is  string indicating text to put in node
  bias.points<-function(bias.x,bias.y,layer.name,cex,...){
    for(val in 1:length(bias.x)){
      points(
        diff(x.range)*bias.x[val],
        bias.y*diff(y.range),
        pch=21,col=in.col,bg=bord.col,cex=circle.cex
      )
      if(node.labs)
        text(
          diff(x.range)*bias.x[val],
          bias.y*diff(y.range),
          paste(layer.name,val,sep=''),
          cex=cex.val
        )
    }
  }
  
  #function creates lines colored by direction and width as proportion of magnitude
  #use 'all.in' argument if you want to plot connection lines for only a single input node
  layer.lines<-function(mod.in,h.layer,layer1=1,layer2=2,out.layer=F,nid,rel.rsc,all.in,pos.col,
                        neg.col,...){
    
    x0<-rep(layer.x[layer1]*diff(x.range)+line.stag*diff(x.range),struct[layer1])
    x1<-rep(layer.x[layer2]*diff(x.range)-line.stag*diff(x.range),struct[layer1])
    
    if(out.layer==T){
      
      y0<-get.ys(struct[layer1])
      y1<-rep(get.ys(struct[layer2])[h.layer],struct[layer1])
      src.str<-paste('out',h.layer)
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts<-wts[grep(src.str,names(wts))][[1]][-1]
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs<-wts.rs[grep(src.str,names(wts.rs))][[1]][-1]
      
      cols<-rep(pos.col,struct[layer1])
      cols[wts<0]<-neg.col
      
      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)
      
    }
    
    else{
      
      if(is.logical(all.in)) all.in<-h.layer
      else all.in<-which(x.names==all.in)
      
      y0<-rep(get.ys(struct[layer1])[all.in],struct[2])
      y1<-get.ys(struct[layer2])
      src.str<-paste('hidden',layer1)
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts<-unlist(lapply(wts[grep(src.str,names(wts))],function(x) x[all.in+1]))
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs<-unlist(lapply(wts.rs[grep(src.str,names(wts.rs))],function(x) x[all.in+1]))
      
      cols<-rep(pos.col,struct[layer2])
      cols[wts<0]<-neg.col
      
      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)
      
    }
    
  }
  
  bias.lines<-function(bias.x,mod.in,nid,rel.rsc,all.out,pos.col,neg.col,...){
    
    if(is.logical(all.out)) all.out<-1:struct[length(struct)]
    else all.out<-which(y.names==all.out)
    
    for(val in 1:length(bias.x)){
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      
    	if(val != length(bias.x)){
        wts<-wts[grep('out',names(wts),invert=T)]
        wts.rs<-wts.rs[grep('out',names(wts.rs),invert=T)]
    		sel.val<-grep(val,substr(names(wts.rs),8,8))
    		wts<-wts[sel.val]
    		wts.rs<-wts.rs[sel.val]
    		}
    
    	else{
        wts<-wts[grep('out',names(wts))]
        wts.rs<-wts.rs[grep('out',names(wts.rs))]
      	}
      
      cols<-rep(pos.col,length(wts))
      cols[unlist(lapply(wts,function(x) x[1]))<0]<-neg.col
      wts.rs<-unlist(lapply(wts.rs,function(x) x[1]))
      
      if(nid==F){
        wts.rs<-rep(1,struct[val+1])
        cols<-rep('black',struct[val+1])
      }
      
      if(val != length(bias.x)){
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1]),
          lwd=wts.rs,
          col=cols
        )
      }
      
      else{
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1])[all.out],
          lwd=wts.rs[all.out],
          col=cols[all.out]
        )
      }
      
    }
  }
  
  #use functions to plot connections between layers
  #bias lines
  if(bias) bias.lines(bias.x,mod.in,nid=nid,rel.rsc=rel.rsc,all.out=all.out,pos.col=alpha(pos.col,alpha.val),
                      neg.col=alpha(neg.col,alpha.val))
  
  #layer lines, makes use of arguments to plot all or for individual layers
  #starts with input-hidden
  #uses 'all.in' argument to plot connection lines for all input nodes or a single node
  if(is.logical(all.in)){  
    mapply(
      function(x) layer.lines(mod.in,x,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,
        all.in=all.in,pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[1]
    )
  }
  else{
    node.in<-which(x.names==all.in)
    layer.lines(mod.in,node.in,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,all.in=all.in,
                pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val))
  }
  #connections between hidden layers
  lays<-split(c(1,rep(2:(length(struct)-1),each=2),length(struct)),
              f=rep(1:(length(struct)-1),each=2))
  lays<-lays[-c(1,(length(struct)-1))]
  for(lay in lays){
    for(node in 1:struct[lay[1]]){
      layer.lines(mod.in,node,layer1=lay[1],layer2=lay[2],nid=nid,rel.rsc=rel.rsc,all.in=T,
                  pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val))
    }
  }
  #lines for hidden-output
  #uses 'all.out' argument to plot connection lines for all output nodes or a single node
  if(is.logical(all.out))
    mapply(
      function(x) layer.lines(mod.in,x,layer1=length(struct)-1,layer2=length(struct),out.layer=T,nid=nid,rel.rsc=rel.rsc,
                              all.in=all.in,pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[length(struct)]
      )
  else{
    node.in<-which(y.names==all.out)
    layer.lines(mod.in,node.in,layer1=length(struct)-1,layer2=length(struct),out.layer=T,nid=nid,rel.rsc=rel.rsc,
                pos.col=pos.col,neg.col=neg.col,all.out=all.out)
  }
  
  #use functions to plot nodes
  for(i in 1:length(struct)){
    in.col<-bord.col<-circle.col
    layer.name<-'H'
    if(i==1) { layer.name<-'I'; in.col<-bord.col<-circle.col.inp}
    if(i==length(struct)) layer.name<-'O'
    layer.points(struct[i],layer.x[i],layer.name)
    }

  if(bias) bias.points(bias.x,bias.y,'B')
  
}

# # # # # # # # # # # # # # #
## Fit LASSO For Predictors

set.seed(4242)

#for lasso
#install.packages("glmnet")
library(glmnet)

train_num <- dplyr::select_if(new_train, is.numeric)

#specify y
y <- train_num$all_liquor_violations

#train$Liquor

exclude_columns <- c("Unitid_all_campus", "OPEID_all_campus", 
                    "Campus.ID_all_campus", "all_liquor_violations", 
                    "Liquor.law.violations_arrests_campus", 
                    "Liquor.law.violations_arrests_public",
                    "Liquor.law.violations_arrests_noncampus",
                    "Liquor.law.violations_arrests_stuhousing",
                    "Liquor.law.violations_disciplinary_campus",
                    "Liquor.law.violations_disciplinary_noncampus",
                    "Liquor.law.violations_disciplinary_public",
                    "Liquor.law.violations_disciplinary_housing",
                    "new_column")

train_finalset <- train_num[, !names(train_num) %in% exclude_columns]

#specify x
x <- data.matrix(train_finalset)


# k fold cv for lambda
cv_model <- cv.glmnet(x,y,alpha = 1)
best_lambda <- cv_model$lambda.min
best_lambda


plot(cv_model, main = "Cross Validation for Lambda")

#find optimal lasso model
best_lasso <- glmnet(x, y, alpha = 1, lambda = best_lambda)

#coefficients from lasso model
lasso_coef <- coef(best_lasso)

lasso_coef

#make coefficients matrix
lc_mat <- as.matrix(lasso_coef)

#make coefficients dataframe
lc_df <- as.data.frame(lc_mat)

#filter out coefficients that are 0
rows_to_keep <- apply(lc_mat, 1, function(row) any(row > 0))

lc_df_filtered <- lc_df[rows_to_keep,]

#remove intercept
lc_df_clean <- lc_df_filtered[-1]

#lc_df_clean

lc_table_df <- data.frame(
  Variable = c("Institution Size", "Sex Offenses (all campus)", "Arson (all campus)", "Rape (student housing)", "Fondling (student housing)", "Robbery (student housing)", "Assault (student housing)", "Burglary (student housing)", "Vehicle Theft (student housing)", "Arson (student housing)", "Assault (criminal offense, noncampus)", "Vehicle Theft (criminal offense, noncampus)", "Arson (criminal offense, noncampus)", "Sex Offenses (criminal offense, public)", "Fondling (criminal offense, public)", "Drug Law Violations (arrest, student housing)", "Drug Law Violations (arrest, noncampus)", "Drug Law Violations (disciplinary, campus)", "Drug Law Violations (disciplinary, housing)"),
  Coefficients = lc_df_clean)

#table of lasso coefficients
knitr::kable(lc_table_df, caption = "LASSO Coefficients", digits = 3)

# # # # # #
## Fit NNs

#install.packages("keras")
library(keras)
library(tensorflow)
library(nnet)

#install.packages("neuralnet")

#compute object is masked from package:dplyr
library(neuralnet)

#get plots side by side, grid.arrange()
#install.packages("gridExtra")
library(gridExtra)

# NN test to see when model breaks
NN_1 <- neuralnet(all_liquor_violations ~ Rape_student_housing + Burglary_student_housing + Arson_student_housing + Drug.law.violations_arrests_noncampus,
                  data = train, hidden = 1, linear.output=TRUE)

NN_2 <- neuralnet(all_liquor_violations ~ Rape_student_housing, hidden = 1, data = train, linear.output = TRUE)

NN_3 <- neuralnet(all_liquor_violations ~ Rape_student_housing + Burglary_student_housing, data = train, hidden = 1, linear.output=TRUE)

NN_4 <- neuralnet(all_liquor_violations ~ Rape_student_housing + Burglary_student_housing, data = train, hidden = c(2, 2), linear.output=TRUE)

NN_5 <- neuralnet(all_liquor_violations ~ Rape_student_housing + Burglary_student_housing + Arson_student_housing, data=train, hidden = 1, linear.output = TRUE)

NN_6 <- neuralnet(all_liquor_violations ~ Rape_student_housing + Burglary_student_housing + Drug.law.violations_arrests_noncampus, data = train, hidden= 1, linear.output=TRUE)

plot(NN_1)

### NN plots

plot.nnet(NN_1, x.lab = c("Rape", "Burglary", "Arson", "Drug LV"), y.lab = "TLV")
title("Model 1")

plot.nnet(NN_2, x.lab = c("Rape"), y.lab = "TLV")
title("Model 2")

plot.nnet(NN_3, x.lab = c("Rape", "Burglary"), y.lab = "TLV")
title("Model 3")

plot.nnet(NN_4, x.lab = c("Rape", "Burglary"), y.lab = "TLV")
title("Model 4")

plot.nnet(NN_5, x.lab = c("Rape", "Burglary", "Arson"), y.lab = "TLV")
title("Model 5")

plot.nnet(NN_6, x.lab = c("Rape", "Burglary", "Drug LV"), y.lab = "TLV")
title("Model 6")


# # # # # # #
## NN RMSEs

library(modelr)

## test rmse

nn_rmse <- data.frame(
  rmse_1 <- rmse(NN_1, data=test),
  rmse_2 <- rmse(NN_2, data=test),
  rmse_3 <- rmse(NN_3, data=test),
  rmse_4 <- rmse(NN_4, data=test),
  rmse_5 <- rmse(NN_5, data=test),
  rmse_6 <- rmse(NN_6, data=test)
)

new_rmse <- t(nn_rmse)

rmse_table <- data.frame(
  Variable = c("1", "2", "3", "4", "5", "6"),
  Coefficients = new_rmse)

rownames(rmse_table) <- NULL

rmse_table

kable(rmse_table, col.names = c("Model #", "Test RMSE"), caption = "Neural Network Model Evaluations", digits = 3)

```

# Conclusion

```{r conclusion}
# # # # # # ## 
# CONCLUSION #
# # # # # # ##

final_rmse <- data.frame(
  Variable = c("XGBoost", "Neural Net"),
  Coefficients = c("164.725", "417.546"))

kable(final_rmse, col.names = c("Method", "Test RMSE"), caption = "Final Model Evaluations", digits = 3)
```


