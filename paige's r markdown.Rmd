---
title: "paige's r markdown"
output: html_document
date: "2024-02-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#libraries
library(dplyr)
library(ggplot2)

# kable
library(knitr)

library(tidyr)
library(tidyverse)
```

## working code from jorge

```{r jorges code for data combination}

getwd()

data1 <- read.csv("Criminal_Offenses_On_campus.csv") |> 
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x,"_all_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_all_campus, unique_id = unique_id_all_campus)

data2 <- read.csv("Criminal_Offenses_On_campus_Student_Housing_Facilities.csv") |> 
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |> 
  rename_with(~ paste0(.x,"_student_housing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_student_housing, unique_id = unique_id_student_housing)

data3 <- read.csv("Criminal_Offenses_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_crim_offense_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_crim_offense_noncampus, unique_id = unique_id_crim_offense_noncampus)

data4 <- read.csv("Criminal_Offenses_Public_property.csv") |>
   mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_crim_offense_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_crim_offense_public, unique_id = unique_id_crim_offense_public)
  
data5 <- read.csv("Arrests_On_campus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_campus, unique_id = unique_id_arrests_campus)

data6 <- read.csv("Arrests_On_campus_Student_Housing_Facilities.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_stuhousing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_stuhousing, unique_id = unique_id_arrests_stuhousing)
  
data7 <- read.csv("Arrests_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_noncampus, unique_id = unique_id_arrests_noncampus)
  
data8 <- read.csv("Arrests_Public_Property.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_arrests_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_arrests_public, unique_id = unique_id_arrests_public)
  
data9 <- read.csv("Disciplinary_Actions_On_campus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_campus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_campus, unique_id = unique_id_disciplinary_campus)

data10 <- read.csv("Disciplinary_Actions_Student_Housing_Facilities.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_housing"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_housing, unique_id = unique_id_disciplinary_housing)

data11 <- read.csv("Disciplinary_Actions_Noncampus.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_noncampus"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_noncampus, unique_id = unique_id_disciplinary_noncampus)
  
data12 <- read.csv("Disciplinary_Actions_Public_Property.csv") |>
  mutate(unique_id = paste0(OPEID, "_", Campus.ID)) |>
  rename_with(~ paste0(.x, "_disciplinary_public"), recycle0 = TRUE) |>
  rename(Survey.year = Survey.year_disciplinary_public, unique_id = unique_id_disciplinary_public)

# This is our datasets being joined into one
dataset <- data1 |> left_join(data2) |>
  left_join(data3) |>
  left_join(data4) |>
  left_join(data5) |>
  left_join(data6) |>
  left_join(data7) |>
  left_join(data8) |>
  left_join(data9) |>
  left_join(data10) |>
  left_join(data11) |>
  left_join(data12) 
```



## data cleaning

removing NA values, removing useless columns

```{r data cleaning}

#remove NAs
dataset[is.na(dataset)] <- 0

#remove repeated columns (like unitid repeating for each xcel file)
#(3/4/24) just fixed some problems w this

cols_to_remove <- c("Unitid_student_housing", "Institution.name_student_housing", "OPEID_student_housing", "Campus.ID_student_housing", "Campus.Name_student_housing", "Institution.Size_student_housing", "Unitid_crim_offense_noncampus", "Institution.name_crim_offense_noncampus", "OPEID_crim_offense_noncampus", "Campus.ID_crim_offense_noncampus", "Campus.Name_crim_offense_noncampus", "Institution.Size_crim_offense_noncampus", "Unitid_crim_offense_public", "Institution.name_crim_offense_public", "OPEID_crim_offense_public", "Campus.ID_crim_offense_public", "Campus.Name_crim_offense_public", "Institution.Size_crim_offense_public", "Unitid_arrests_campus", "Institution.name_arrests_campus", "OPEID_arrests_campus", "Campus.ID_arrests_campus", "Campus.Name_arrests_campus", "Institution.Size_arrests_campus", "Unitid_arrests_stuhousing", "Institution.name_arrests_stuhousing", "OPEID_arrests_stuhousing", "Campus.ID_arrests_stuhousing", "Campus.Name_arrests_stuhousing", "Institution.Size_arrests_stuhousing", "Unitid_arrests_noncampus", "Institution.name_arrests_noncampus", "OPEID_arrests_noncampus", "Campus.ID_arrests_noncampus", "Campus.Name_arrests_noncampus", "Institution.Size_arrests_noncampus", "Unitid_arrests_public", "Institution.name_arrests_public", "OPEID_arrests_public", "Campus.ID_arrests_public", "Campus.Name_arrests_public", "Institution.Size_arrests_public", "Unitid_disciplinary_campus", "Institution.name_disciplinary_campus", "OPEID_disciplinary_campus", "Campus.ID_disciplinary_campus", "Campus.Name_disciplinary_campus", "Institution.Size_disciplinary_campus", "Unitid_disciplinary_noncampus", "Institution.name_disciplinary_noncampus", "OPEID_disciplinary_noncampus", "Campus.ID_disciplinary_noncampus", "Campus.Name_disciplinary_noncampus", "Institution.Size_disciplinary_noncampus", "Unitid_disciplinary_public", "Institution.name_disciplinary_public", "OPEID_disciplinary_public", "Campus.ID_disciplinary_public", "Campus.Name_disciplinary_public", "Institution.Size_disciplinary_public", "Unitid_disciplinary_housing", "Institution.name_disciplinary_housing", "OPEID_disciplinary_housing", "Campus.ID_disciplinary_housing", "Campus.Name_disciplinary_housing", "Institution.Size_disciplinary_housing")
cleaned_data <- dataset[, !names(dataset) %in% cols_to_remove]
```


## tests

```{r tests}
mean(cleaned_data$Institution.Size_all_campus)
```
## summary stats

```{r sum stat 1}

#new column combining liquor law violations across disciplinary, arrests and location (public, stuhousing, campus, noncampus)
cleaned_data$all_liquor_violations <- cleaned_data$Liquor.law.violations_arrests_campus + cleaned_data$Liquor.law.violations_arrests_noncampus + cleaned_data$Liquor.law.violations_arrests_public + cleaned_data$Liquor.law.violations_arrests_stuhousing + cleaned_data$Liquor.law.violations_disciplinary_campus + cleaned_data$Liquor.law.violations_disciplinary_housing + cleaned_data$Liquor.law.violations_disciplinary_noncampus + cleaned_data$Liquor.law.violations_disciplinary_public

numeric_data <- select(cleaned_data, where(is.numeric))

# figure margins too large
#pairs(numeric_data)

ggplot(cleaned_data, aes(y=Institution.Size_all_campus, x=all_liquor_violations)) +
  geom_point() +
  geom_smooth(method = "lm")

#cleaned_data$all_liquor_violations[16]

year_factor <- as.factor(cleaned_data$Survey.year)

#ggplot(cleaned_data, aes(x=year_factor, y=all_liquor_violations)) + 
 # geom_bar(stat= "identity", aes(fill=year_factor)) +
  #xlab("Year") +
  #ylab("Liquor Law Violations") +
  #ggtitle("Barplot of Total Liquor Violations v. Year")

ggplot(cleaned_data, aes(x = year_factor, y = all_liquor_violations, fill = year_factor)) +
  geom_bar(stat = "identity") +
  labs(x = "Year", y = "Liquor Law Violations", fill = "Year") +
  ggtitle("Barplot of Total Liquor Violations vs. Year")


#ggplot(cleaned_data, aes(Institution.Size_all_campus)) +
 # geom_histogram() +
 # xlab("Institution Enrollment") +
 # ylab("Count of Campuses") +
 # ggtitle("Histogram of Institution Size")

#small_inst <- cleaned_data$Institution.Size_all_campus <= 15000
#large_inst <- cleaned_data$Institution.Size_all_campus >= 15001
#large_inst <- cleaned_data$Institution.Size_all_campus >= 20000

#size_data <- data.frame()

#size_table <- data.frame(
 # Small = sum(small_inst),
 # Large = sum(large_inst)
#)

#kable(size_table, caption = "Institution Sizes")

#ggplot(cleaned_data, aes(y=all_liquor_violations, x=unique_id)) +
  #geom_boxplot()

#summary(lm(all_liquor_violations ~ ., data= cleaned_data)) +
#knitr::kable(digits=c(3,3,3,3),
#caption ="Simple Linear Regression Model Estimating Liquor Violations from All Predictors")

```
## K-fold help: code from DSCI 445

```{r}
## Libraries
library(ISLR) ## data
library(tidyverse) ## data manipulation & plots
library(tidymodels) ## tidy models

## Data
head(Auto)

## Reproducibility
set.seed(445)

## Validation Set Approach
# 1. Split the data into 50% training and 50% test data.
Auto.val <- validation_split(Auto, prop = 0.5)

# 2. Fit a linear model of `mpg` on `horsepower` using your training data.
linear_spec <- linear_reg()
linear_rec <- recipe(mpg ~ horsepower, data = Auto)

linear_model <- workflow() |>
  add_model(linear_spec) |>
  add_recipe(linear_rec) |>
  fit_resamples(resamples = Auto.val)


# 3. Estimate the test error by using test MSE.
linear_model |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

# 4. Repeat steps 2-3 for a cubic and quadratic model. Which model would you pick?
quad_rec <- linear_rec |> step_mutate(horsepower2 = horsepower^2)

workflow() |>
  add_model(linear_spec) |>
  add_recipe(quad_rec) |>
  fit_resamples(resamples = Auto.val) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

cubic_rec <- quad_rec |> step_mutate(horsepower3 = horsepower^3)

workflow() |>
  add_model(linear_spec) |>
  add_recipe(cubic_rec) |>
  fit_resamples(resamples = Auto.val) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## I would pick the quadratic model, although the cubic is very close to the same test MSE.

# 5. Repeat steps 1-4 after reseting the seed
set.seed(42)

## split data
Auto.val2 <- validation_split(Auto, prop = 0.5)

## linear
workflow() |>
  add_model(linear_spec) |>
  add_recipe(linear_rec) |>
  fit_resamples(resamples = Auto.val2) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## quadratic
workflow() |>
  add_model(linear_spec) |>
  add_recipe(quad_rec) |>
  fit_resamples(resamples = Auto.val2) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## cubic
workflow() |>
  add_model(linear_spec) |>
  add_recipe(cubic_rec) |>
  fit_resamples(resamples = Auto.val2) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)


# 6. Did you get the same results? Is this what you expected to happen?

## yes, same results (quadratic is the best model), although the estimates of error are not exactly the same.
## I would expect the results to be slightly different because the validation approach has a lot of variance 
## => the estimates depend heavily on the exact split.

## LOOCV

# 1. Get the estimate of test MSE for the linear model using LOOCV.

## split data
Auto.loo <- vfold_cv(Auto, v = nrow(Auto))

## linear
workflow() |>
  add_model(linear_spec) |>
  add_recipe(linear_rec) |>
  fit_resamples(resamples = Auto.loo) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## 14.81286

# 2. Repeat steps 2-3 for a cubic and quadratic model. Which model would you pick?

## quadratic
workflow() |>
  add_model(linear_spec) |>
  add_recipe(quad_rec) |>
  fit_resamples(resamples = Auto.loo) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## 10.70625

## cubic
workflow() |>
  add_model(linear_spec) |>
  add_recipe(cubic_rec) |>
  fit_resamples(resamples = Auto.loo) |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  mutate(mean = mean^2) |>
  pull(mean)

## 10.73746

## I would still pick the quadratic model as it has the lowest estimates test MSE



### this is the part we want to use ###

## k-Fold CV
# 1. Using $k = 10$-fold CV, compute the $k$-fold CV estimate of the test MSE for polynomial models of order $i = 1, \dots, 10$. (Hint: you can use the `poly` function in your formula to specify a polynomial model.)
Auto.kfold <- vfold_cv(Auto, v = 10) ## split into k folds
degrees <- data.frame(degree = 1:10) ## tuning values

## setup polynomial regressions
poly_rec <- recipe(mpg ~ horsepower, data = Auto) |>
  step_poly(horsepower, degree = tune("degree"))

workflow() |>
  add_model(linear_spec) |>
  add_recipe(poly_rec) |>
  tune_grid(resamples = Auto.kfold, grid = degrees) -> tune.fit

# 2. Plot the estimated test MSE vs. the polynomial order.
collect_metrics(tune.fit) |>
  select(degree, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  mutate(mse = rmse^2) |>
  ggplot() +
  geom_line(aes(degree, mse)) +
  geom_point(aes(degree, mse))

# 3. Which of these models would you choose?
# I would pick the model with polynomial of degree 7
show_best(tune.fit, metric = "rmse")

## Bonus
# 1. Write your own $k$-fold CV function that will calculate CV for the $KNN$ Regression model. You function should take as parameters
#    - CV $k$ value
#    - KNN $K$ value
#    - Data
#    - A vector of names (character) of predictor columns
#    - A character string of the response column
# And return the estimated test MSE.
# 2. Use your function to estimate the test MSE using 10-fold CV for KNN models with $K = 1, 5, 10, 20, 100$ of a model predicting `mpg` using the `horsepower` predictor variable in the `Auto` data set.
# 3. Compare your results to the previous $k$-Fold CV method.
k_fold_cv_err_knn <- function(k_fold = 10, knn, data, formula) {
  data.kfold <- vfold_cv(data, v = k_fold)
  
  knn_spec <- nearest_neighbor(mode = "regression", neighbors = knn)
  knn_res <- recipe(formula, data)
  
  workflow() |>
    add_model(knn_spec) |>
    add_recipe(knn_res) |>
    fit_resamples(data.kfold) |>
    collect_metrics() |>
    select(.metric, mean) |>
    pivot_wider(names_from = .metric, values_from = mean) |>
    mutate(mse = rmse^2) |>
    pull(mse)
}

res <- data.frame(knn = c(1, 5, 10, 20, 100))
for(i in seq_len(nrow(res))) {
  res[i, "mse"] <- k_fold_cv_err_knn(10, res[i, "knn"], Auto, mpg ~ horsepower)
}  

collect_metrics(tune.fit) |>
  select(degree, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  mutate(mse = rmse^2) |>
  mutate(model = "lm") |>
  select(model, degree, mse) |>
  bind_rows(res |> rename(degree = knn) |> mutate(model = "knn")) |>
  ggplot() +
  geom_line(aes(degree, mse)) +
  geom_point(aes(degree, mse)) +
  facet_grid(. ~ model, scales = "free_x")

res[which.min(res$mse),]

show_best(tune.fit, metric = "rmse") |> mutate(mse = mean^2)

## knn with 100 neighbors looks better than the best polynomial model.
## based on the above I would choose the model with neighbor size of 20 or 100. We know as K gets 
## larger there is more of a linear decision boundary. This is at odds with our polynomial regression
## results, which pointed to needing a highly flexible model. However, whether KNN or polynomial regression,
## it appears we achieve similar estimated test MSE.
```


## this code is useless

```{r load files}

#read CSV files

#arrests_local_state_police <- read.csv("C:/Users/paige/Downloads/OPE CSS Custom Data 2024-02-28 171746/Arrests_Local_State_Police.csv")

#arrests_noncampus <- read.csv("C:/Users/paige/Downloads/OPE CSS Custom Data 2024-02-28 171746/Arrests_Noncampus.csv")

#arrests_oncampus <- read.csv("C:/Users/paige/Downloads/OPE CSS Custom Data 2024-02-28 171746/Arrests_On_campus.csv")

##problem: need to combine based on university and campus - make new id

# Create a new column by combining "userID" and "campusID"
#arrests_local_state_police$new_ID <- paste(arrests_local_state_police$Unitid, arrests_local_state_police$Campus.ID, sep = "-")

#arrests_noncampus$new_ID <- paste(arrests_noncampus$Unitid, arrests_noncampus$Campus.ID, sep = "-")

# Add source column

#arrests_local_state_police$source <- "Local_State_Police"
#arrests_noncampus$source <- "Noncampus"

# combine data frames

#arrests_combined_1 <- rbind(arrests_local_state_police, arrests_noncampus)

#arrests_combined_1 <- bind_rows(arrests_local_state_police, arrests_noncampus)

# write combined data to CSV

#write.csv(arrests_combined_1, "arrests_combined_1.csv", row.names=FALSE)
```